{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3\n",
    "\n",
    "Measure the performance of several database operations, and compare it to the theoretical time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The time for all operations up to 10,000 records:\n",
    "\n",
    "![plot_with_loadsales](lab3_loadsales.png)\n",
    "\n",
    "The time without `load_sales`, so we can actually see the others:\n",
    "\n",
    "![plot_without_loadsales](lab3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Trends:\n",
    "\n",
    "I noticed that all the operations grow mostly linearly, with some variation. There is a strange spike in `check_duplicates` around 80000, and `search_sale` is highly random.\n",
    "This is aligned to my expectations; All the operations are `O(n)`, although there is some ambiguity in the time complexity of the under-the-hood python internals (i.e. Does python use lists or arrays? Are sets actually constant-time to check inclusion as I assumed?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Implications:\n",
    "\n",
    "The largest and most obvious bottleneck is file handling. `load_sales` completely dwarfs the other times.\n",
    "\n",
    "None of the approaches are quadratic, so that's not a consideration, but one optimization you could do is use dictionary, which could easily achieve constant-time for most of the operations, and constant-time amortized for `check_duplicates` by the simple fact that duplicates can't exist.\n",
    "\n",
    "And to optimize `load_sales`, although likely not in an theoretical-asymptotic way, one could use a more specialized database, such as a SQL database (although this would turn a lot of the other operations into simply API queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Adjustments\n",
    "\n",
    "A test plan for the project would likely be a small example set of data (maybe using a set seed for the random generator), and manually deduce the expected outcomes of each operation.\n",
    "\n",
    "Meanwhile, my current program does no error handling. Any improperly formated data would lead to a runtime error. A simple data validation step during loading would help solve this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
